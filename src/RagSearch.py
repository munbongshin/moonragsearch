from dotenv import load_dotenv
from pathlib import Path
import streamlit as st
import sys, platform
import asyncio
from langchain_core.messages import ChatMessage
from langchain_core.documents import Document
#from langchain.llms import Ollama
from langchain.chains.question_answering import load_qa_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from sentence_transformers import SentenceTransformer
import os, base64, time
import subprocess, requests
from langchain_community.llms import Ollama
from langchain_groq import ChatGroq  #pip install langchain-groq
from langchain_openai import ChatOpenAI
from TextSummarizer import TextSummarizer #by mbs
from GroqManager import  GroqManager
from GroqManager import get_groq_response as GroqResponse
from ChromaDbManager import ChromaDbManager
from CustomSentenceTransformerEmbeddings import CustomSentenceTransformerEmbeddings as CSTFM

st.set_page_config(page_title="AIì—ê²Œ ì§ˆë¬¸í•˜ê¸°", layout="wide")

FILLTERED_DOC_NUMBER = 5

#@st.cache_data
def load_embeddings():
    return CSTFM()

@st.cache_resource
def load_llm():
    project_root = Path(__file__).parent.parent
    env_path = project_root / '.env'
    load_dotenv(dotenv_path=env_path)
    
    baseurl = os.environ.get("BASE_URL")
    modelllm = os.environ.get("DEFAULT_LLMNAME")
    api_key = os.environ.get("API_KEY", "lm-studio")  # API í‚¤ë„ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜´

    if not all([baseurl, modelllm]):
        raise ValueError("í•„ìš”í•œ í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    chatllm = ChatOpenAI(
        base_url=baseurl,
        api_key=api_key,        
        model=modelllm,
        streaming=True,
        temperature=0,
    )
    
    return chatllm

class RAGChatAppUI:
    def __init__(self, app):
        self.app = app
        self.llm_name = "LM Studio"
        
    def display_chat_messages(self):
        for message in st.session_state.messages:
            with st.chat_message(message.role):
                st.markdown(message.content)

    def setup_main_ui(self):

        st.header("Ask your PDF, DOC, PPT, Excel, HWP ğŸ’¬")
        
        main_content = st.empty()
        with main_content.container():
            self.maincol1, self.maincol2 = st.columns([7, 3])
            
    def set_llm_name(self, llm_name):
        self.llm_name =  llm_name

    def setup_sidebar(self):
        with st.sidebar:
            st.header("Rag Menu")
            tab1, tab2, tab3, tab4 = st.tabs(["Arg Chat", "ì„ë² ë”©", "ChromaDB ê´€ë¦¬", "LLM ëª¨ë¸"])
            
            with tab1:
                self.setup_arg_chat_tab()
            with tab2:
                self.setup_embed_tab()
            with tab3:
                self.setup_chromadb_tab()
            with tab4:
                self.setup_llmmodel_tab()

    def setup_arg_chat_tab(self):
        st.header("Arg Chat")
        st.session_state.rag_usage = st.radio("Ragì´ìš©ì—¬ë¶€ ğŸ‘‡", ["Rag+LLM", "LLM"], index=0 if st.session_state.rag_usage == "Rag+LLM" else 1)
        collections = self.app.db_manager.list_collections()
        selected_collection = st.selectbox("ê²€ìƒ‰í•  ì»¬ë ‰ì…˜ì„ ì„ íƒí•˜ì„¸ìš”", collections, key="search_collection_name")
        self.app.set_collection_name(selected_collection)
        self.setup_document_selection(selected_collection)

    def setup_document_selection(self, collection_name):
        st.subheader("ê²€ìƒ‰ëŒ€ìƒ ë¬¸ì„œì„ íƒ")
        source_search = st.text_input("ë¬¸ì„œê²€ìƒ‰:", key="source_search_input")
        
        if st.button("ê²€ìƒ‰"):
            st.session_state.filtered_sources = self.app.db_manager.get_all_documents_source(collection_name, source_search)
            st.session_state.show_search_results = True
        
        if st.session_state.show_search_results:
            with st.expander("ê²€ìƒ‰ ê²°ê³¼", expanded=True):
                st.write("ë‹¤ìŒ ë¬¸ì„œë“¤ì´ ê²€ìƒ‰ë˜ì—ˆìŠµë‹ˆë‹¤. ì„ íƒí•˜ì„¸ìš”:")
                for source in st.session_state.filtered_sources:
                    if st.checkbox(source, key=f"check_{source}"):
                        if source not in st.session_state.selected_sources:
                            st.session_state.selected_sources.append(source)
                    elif source in st.session_state.selected_sources:
                        st.session_state.selected_sources.remove(source)                
                if st.button("ì„ íƒ ì™„ë£Œ"):
                    st.session_state.show_search_results = False
                    
        selected = st.multiselect(
            "ì„ íƒëœ ë¬¸ì„œ:", 
            options=st.session_state.filtered_sources,
            default=st.session_state.selected_sources,
            key="final_selected_sources",
        )
        
        st.session_state.selected_sources = selected
        
        col1, col2 = st.columns(2)
        with col1: 
            st.session_state.apply_filter = st.checkbox("ì ìš©", value=st.session_state.apply_filter)
        with col2:
            if st.button("í•„í„° ì´ˆê¸°í™”"):
                st.session_state.selected_sources = []
                st.session_state.filtered_sources = []
                st.session_state.apply_filter = False

    def setup_embed_tab(self):
        st.header("ì„ë² ë”©")         
        collections = self.app.db_manager.list_collections()       
        selected_collection = st.selectbox("ì»¬ë ‰ì…˜ì„ ì„ íƒí•˜ì„¸ìš”", collections, key="embed_collection_select")
        
        if st.button("íŒŒì¼ ì—…ë¡œë” ì´ˆê¸°í™”"):
            st.session_state.files_processed = False
            st.rerun()

        if not st.session_state.files_processed:
            uploaded_files = st.file_uploader("íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”", type=['.pdf','.pptx','.ppt', \
                '.doc', '.docx','.hwp','.hwpx','.xlsx','.md','.txt','html', '.htm'], accept_multiple_files=True)
            
            if st.button("ë²¡í„°ì €ì¥", key="store_vector_collection"):            
                if uploaded_files is not None:                
                    total_chunks_stored = 0
                    for file in uploaded_files:
                        if self.app.db_manager.check_source_exists(selected_collection, file):
                           st.info(f"{file.name} íŒŒì¼ì´ DBì— ì´ë¯¸ ì¡´ì¬í•˜ì—¬ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                           time.sleep(5)
                        else:                            
                            try:
                                with st.spinner(f"{file.name} íŒŒì¼ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..."):
                                    text = self.app.db_manager.extract_text_from_file(
                                        file, file.name)
                                
                                if not text:
                                    st.warning(f"{file.name}ì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
                                    continue

                                with st.spinner(f"{file.name} í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë° ì„ë² ë”© ì¤‘..."):
                                    processed_text = text
                                    chunks_stored = self.app.db_manager.split_embed_docs_store(
                                        processed_text, file.name, selected_collection)
                                
                                total_chunks_stored += chunks_stored
                                st.success(f"'{file.name}' íŒŒì¼ì´ ì²˜ë¦¬ë˜ì–´ {chunks_stored}ê°œì˜ ì²­í¬ê°€ '{selected_collection}' ì»¬ë ‰ì…˜ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                            except Exception as e:
                                st.error(f"{file.name} íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                                time.sleep(5)
                    
                    st.success(f"ì´ {total_chunks_stored}ê°œì˜ ì²­í¬ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    st.session_state.files_processed = True
                    st.rerun()
        else:
            st.success("íŒŒì¼ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë ¤ë©´ 'íŒŒì¼ ì—…ë¡œë” ì´ˆê¸°í™”' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")

    def setup_chromadb_tab(self):
        st.title("Arg Searchë¥¼ ìœ„í•œ ChromaDB ê´€ë¦¬ Section")
        management_menu = st.selectbox(
            "ì‘ì—…ì„ ì„ íƒí•˜ì„¸ìš”",
            ("Collection ìƒì„±", "Collection ì‚­ì œ", "Collection ë‚´ìš©ê²€ìƒ‰", "Collection ë‚´ìš©ë³´ê¸°"),
            key="tabmenu_select"
        )
        collections = self.app.db_manager.get_list_collections()
        
        if management_menu == "Collection ìƒì„±":
            self.create_collection_ui()
        elif management_menu == "Collection ì‚­ì œ":
            self.delete_collection_ui(collections)
        elif management_menu == "Collection ë‚´ìš©ê²€ìƒ‰":
            self.search_collection_ui(collections)
        elif management_menu == "Collection ë‚´ìš©ë³´ê¸°":
            self.view_collection_ui(collections)

    def create_collection_ui(self):
        st.header("ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±")
        collection_name = st.text_input("ìƒì„±í•  ì»¬ë ‰ì…˜ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”")                
        if st.button("ìƒì„±", key="create_new_collection"):
            if collection_name not in self.app.db_manager.get_list_collections():
                result = self.app.db_manager.create_collection(collection_name)
                st.info(f"ì»¬ë ‰ì…˜ '{result}'ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                st.info(f"ì»¬ë ‰ì…˜ '{collection_name}'ì´ ì¡´ì¬í•˜ì—¬ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    def delete_collection_ui(self, collections):
        st.header("ì»¬ë ‰ì…˜ ì‚­ì œ")
        if 'delete_state' not in st.session_state:
            st.session_state.delete_state = 'initial'

        selected_collection = st.selectbox("ì‚­ì œí•  ì»¬ë ‰ì…˜ì„ ì„ íƒí•˜ì„¸ìš”", collections, key="delete_collection_select")

        if st.button("ì‚­ì œ", key="delete_collection"):
            st.session_state.delete_state = 'confirm'
                    
        if st.session_state.delete_state == 'confirm':
            st.write(f"'{selected_collection}' ì»¬ë ‰ì…˜ì„ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
            col1, col2 = st.columns(2)
            with col1:
                if st.button("ì˜ˆ", key="confirm_yes"):
                    result = self.app.db_manager.delete_collection(selected_collection)
                    st.success(f"'{selected_collection}' ì»¬ë ‰ì…˜ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    st.session_state.delete_state = 'initial'
                    st.rerun()        
                        
            with col2:
                if st.button("ì•„ë‹ˆì˜¤", key="confirm_no"):
                    st.info("ì‚­ì œê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    st.session_state.delete_state = 'initial'
                    st.rerun()        

    def search_collection_ui(self, collections):
        st.header("ì»¬ë ‰ì…˜ ê²€ìƒ‰")
        selected_collection = st.selectbox("ê²€ìƒ‰í•  ì»¬ë ‰ì…˜ì„ ì„ íƒí•˜ì„¸ìš”", collections, key="search_collection_select")
        search_query = st.text_input("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
        if st.button("ê²€ìƒ‰", key="search_collectison_content"):
            results = self.app.db_manager.search_collection(selected_collection, search_query, self.app.db_manager.docnum, inscore=350)                
            self.display_search_results(results)  # self.app.display_search_results(results) ëŒ€ì‹ 


    def view_collection_ui(self, collections):
        st.header("ì»¬ë ‰ì…˜ ë‚´ìš©ë³´ê¸°")
        selected_collection = st.selectbox("ì»¬ë ‰ì…˜ì„ ì„ íƒí•˜ì„¸ìš”", collections, key="view_collection_select")            
        if st.button("ë‚´ìš© ë³´ê¸°", key="view_collection_content"):
            content = self.app.db_manager.view_collection_content(selected_collection)
            st.write(f"'{selected_collection}'ì˜ ë‚´ìš©ì…ë‹ˆë‹¤.")
            st.markdown(content)

    def setup_llmmodel_tab(self):

        llm_source = st.radio("LLM ì†ŒìŠ¤ ì„ íƒ:", ("LM Studio", "Ollama","Groq"))

        if llm_source == "Ollama":
            models = self.app.get_ollama_models()
            self.set_llm_name(llm_name="Ollama")
            if not models:
                st.error("ì‚¬ìš© ê°€ëŠ¥í•œ Ollama ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. 'ollama pull' ëª…ë ¹ì–´ë¡œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œ í•´ì£¼ì„¸ìš”.")
                self.app.set_llm_model(self.app.lm_llm)
                return 

            selected_model = st.selectbox("ì‚¬ìš©í•  Ollama ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:", models)
            st.write(f"ì„ íƒí•œ ëª¨ë¸: {selected_model}")

            llm = Ollama(model=selected_model)
            self.app.set_llm_model(llm)
        elif llm_source == "LM Studio":  # LM Studio
            models = self.app.get_lm_studio_models()       
            self.set_llm_name(llm_name="LM Studio")
            if models:
                self.app.lm_llm.model_name = models
                self.app.set_llm_model(self.app.lm_llm)
                st.success("LM Studioì—ì„œ ë™ì¼í•œ ëª¨ë¸ì´ ì‹¤í–‰ë˜ê³  ìˆì–´ì•¼ ì •ìƒì ìœ¼ë¡œ ì‘ë™ë©ë‹ˆë‹¤.")
            else:
                self.app.set_llm_model(self.app.lm_llm)
                st.error("ë””í´íŠ¸ ì…‹íŒ…ì„ ë”°ë¦…ë‹ˆë‹¤.")
        else:
            models = self.app.load_groq()
            self.set_llm_name(llm_name="Groq")            
            if models:
                self.app.lm_llm.model_name = models
                self.app.set_llm_model(self.app.lm_llm)
                st.success("Groq(ì™¸ë¶€API)ì„ ì´ìš©í•©ë‹ˆë‹¤.")
            else:
                self.app.set_llm_model(self.app.lm_llm)
                st.error("ë””í´íŠ¸ ì…‹íŒ…ì„ ë”°ë¦…ë‹ˆë‹¤.")

    def display_chat_interface(self):
        with self.maincol1:           
            input_container = st.container()
            chat_history = st.container()
            # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
            with input_container:
                user_question = st.chat_input("Enter your search query")
                if user_question:
                    st.session_state.messages.append(ChatMessage(role="user", content=user_question))
                    asyncio.run(self.app.process_query_async(user_question))
                    self.update_chat_history(chat_history)
            
    def display_chat_messages(self):
        for message in st.session_state.messages:
            with st.chat_message(message.role):
                st.markdown(message.content)
    
    def update_chat_history(self, chat_history):
        with chat_history:
            for message in reversed(st.session_state.messages):
                if message.role == "user":
                    st.chat_message("user").write(message.content)
                else:
                    st.chat_message("assistant").write(message.content)
    

    def display_search_results(self, results):
        with self.maincol2:
            if not results:
                st.write("No results found.")
            else:
                for i, result in enumerate(results, 1):
                    key = "view_" + str(i)
                    with st.expander(f"Result {i}"):
                        if isinstance(result, dict):
                            st.write(f"**Content:** {result['page_content']}")
                            st.write(f"**Metadata:** {result['metadata']}")
                            st.write(f"**Score:** {result['score']:.1f}")
                            if st.button(f"**File_name:** {result['metadata']['file_name']}", key=key):
                                self.app.show_pdf(result['metadata']['file_name'])
                        else:
                            st.write(f"**Content:** {result.page_content}")
                            st.write(f"**Metadata:** {result.metadata}")
                            st.write(f"**Score:** {result.metadata.get('score', 'N/A'):.1f}")
                            if st.button(f"**File_name:** {result.metadata.get('file_name', 'Unknown')}", key=key):
                                self.app.show_pdf(result.metadata.get('file_name'))

class RAGChatApp:
    def __init__(self):
        project_root = Path(__file__).parent.parent
        # .env íŒŒì¼ì˜ ê²½ë¡œ ì„¤ì •
        env_path = project_root / '.env'
        load_dotenv(dotenv_path=env_path)
        self.chromadb = ChromaDbManager()
        self.persist_directory = self.chromadb.get_persist_directory;
        self.collection_name = "rag_test"
        self.db_manager = self.chromadb
        self.lm_llm = load_llm()
        self.llm = self.lm_llm
        self.initialize_session_state()
        self.ui = RAGChatAppUI(self)
        os.environ["POSTHOG_DISABLED"] = "1"
        self.embeddings = load_embeddings()
        self.models = self.load_models_from_env("LM")
        self.groq = GroqManager()
        self.groq_models = self.load_models_from_env("GROQ")
        
    async def process_query_async(self, query):
        keyword, parsed_query = self.parse_input(query)
        if (keyword == "f" or st.session_state.apply_filter) and st.session_state.rag_usage == "Rag+LLM":
            await self.process_filtered_query_async(parsed_query)
        else:
            await self.process_regular_query_async(parsed_query)

    async def process_filtered_query_async(self, query):
        if st.session_state.selected_sources:
            docs = await asyncio.to_thread(self.db_manager.get_documents_by_source, 
                                           self.collection_name, st.session_state.selected_sources)
            if docs:
                await self.generate_response_async(docs, query)
            else:
                st.write("No documents found for the selected sources.")
        else:
            st.info("ê²€ìƒ‰í•  ë¬¸ì„œë¥¼ ì„ íƒí•˜ì„¸ìš”!")

    async def process_regular_query_async(self, query):
        if st.session_state.RagUsage == "Rag+LLM":
            with st.spinner('Rag+LLM ë‹µë³€ ì‚¬ì „ ì¤€ë¹„ì¤‘...'):
                docs = await asyncio.to_thread(self.perform_search, query, self.db_manager, 
                                               self.collection_name, st.session_state.selected_sources)
            if docs:
                await self.generate_response_async(docs, query)
                self.ui.display_search_results(docs)
            else:
                await self.fallback_to_llm_async(query)
        else:
            await self.generate_response_async(None, query)

    async def generate_response_async(self, docs, query):
        try:
            
            chain = load_qa_chain(self.llm, chain_type="stuff", verbose=True)
            with st.spinner('AI ë‹µë³€ì„ ê¸°ë‹¤ë¦¬ê³  ìˆëŠ”ì¤‘...'):
                if self.ui.llm_name == "Groq":
                    docs_list = [docs] if isinstance(docs, str) else docs
                    response = GroqResponse(self.groq, docs_list, query)
                    content = response['content'] if isinstance(response['content'], str) else str(response['content'])
                    response = content
                else:
                    response = await asyncio.to_thread(chain.run, input_documents=docs or "", question=query)
                st.session_state.messages.append(ChatMessage(role="assistant", content=response))
        except Exception as e:
            error_message = f"LLM ì„œë²„ì—°ê²° ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)
            st.session_state.messages.append(ChatMessage(role="assistant", content=error_message))

    async def fallback_to_llm_async(self, query):
        st.info("ì„ íƒí•œ ì†ŒìŠ¤ë‚˜ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ LLMì— ì§ˆì˜í•©ë‹ˆë‹¤.")
        await self.generate_response_async("", query)

    def initialize_session_state(self):
        
        if "messages" not in st.session_state:
            st.session_state.messages = []
        if "docs" not in st.session_state:
            st.session_state.docs = None
        if "selected_sources" not in st.session_state:
            st.session_state.selected_sources = []
        if "last_query" not in st.session_state:
            st.session_state.last_query = ""
        if "rag_usage" not in st.session_state:
            st.session_state.rag_usage = "Rag+LLM"
        if "max_docnum" not in st.session_state:
            st.session_state.max_docnum = 3
        if "apply_filter" not in st.session_state:
            st.session_state.apply_filter = False
        if "filtered_sources" not in st.session_state:
            st.session_state.filtered_sources = []
        if "RagUsage" not in st.session_state:    
            st.session_state.RagUsage = "Rag+LLM"
            
                # ìƒˆë¡œìš´ ì´ˆê¸°í™” ì½”ë“œ ì¶”ê°€
        if "show_search_results" not in st.session_state:
            st.session_state.show_search_results = False
        if "files_processed" not in st.session_state:
            st.session_state.files_processed = False

    def set_collection_name(self, ragname):
        self.collection_name = ragname

    def set_llm_model(self, modelname):
        self.llm = modelname

    def get_ollama_models(self):
        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)
        lines = result.stdout.strip().split('\n')[1:]
        models = [line.split()[0] for line in lines]
        return models
    
    def load_groq(self):
        # .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
        load_dotenv()
        
        # GROQ_API_KEY í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
        api_key = os.getenv("GROQ_API_KEY")        
        if not api_key:
            raise ValueError("GROQ_API_KEYê°€ í™˜ê²½ ë³€ìˆ˜ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
       
        # ì‚¬ìš©í•  ëª¨ë¸ ì§€ì •
        models = self.get_groq_models()       
        self.ui.set_llm_name(llm_name="Groq")
        
        if models:
            self.lm_llm.model_name = models  # ì²« ë²ˆì§¸ ëª¨ë¸ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
            self.set_llm_model(self.groq)
        else:
            st.error("ì‚¬ìš© ê°€ëŠ¥í•œ Groq ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None  # ëª¨ë¸ì´ ì—†ìœ¼ë©´ None ë°˜í™˜
        # Groq í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        try:
            groq_llm = ChatGroq(
                groq_api_key=api_key,
                model_name=models,  # Noneì´ ì•„ë‹Œ ì‹¤ì œ ëª¨ë¸ ì´ë¦„ ì‚¬ìš©
                temperature=0,
            )
            return groq_llm
        except Exception as e:
            st.error(f"Groq í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return None
    
    def load_models_from_env(self,name):
        models = {}
        for key, value in os.environ.items():
            if name == "LM":
                if key.startswith('LM_STUDIO_MODEL_'):
                    model_name = key.replace('LM_STUDIO_MODEL_', '')
                    models[model_name] = value
            if name == "GROQ":
                if key.startswith('GROQ_MODEL_'):
                    model_name = key.replace('GROQ_MODEL_', '')
                    models[model_name] = value            
        return models

    def get_lm_studio_models(self):
        if not self.models:
            st.error("No LM Studio models defined in .env file.")
            return None

        selected_model = st.selectbox(
            "Select LM Studio Model",
            options=list(self.models.keys()),
            format_func=lambda x: f"{x} ({self.models[x]})"
        )

        if selected_model:
            model_id = self.models[selected_model]
            return model_id
        else:
            return None
    
    def get_groq_models(self):
        if not self.groq_models:
            st.error("No Groq models defined in .env file.")
            return None

        selected_groqmodel = st.selectbox(
            "Select Groq Model",
            options=list(self.groq_models.keys()),
            format_func=lambda x: f"{x} ({self.groq_models[x]})"
        )

        if selected_groqmodel:
            model_id = self.groq_models[selected_groqmodel]
            return model_id
        else:
            return None
    
    def parse_input(self, input_text):
        parts = input_text.split(']', 1)
        if len(parts) > 1 and parts[0].startswith('['):
            keyword = parts[0][1:].strip().lower()
            query = parts[1].strip()
            return keyword, query
        else:
            return None, input_text.strip()

    def perform_search(self, query, db_manager, collection_name, selected_sources=None):
        raw_results = db_manager.search_collection(collection_name, query, n_results=FILLTERED_DOC_NUMBER, inscore=350)
        if not raw_results:
            return []
        docs = []
        for result in raw_results:
            if not isinstance(result['page_content'], str):
                result['page_content'] = str(result['page_content'])
            
            doc = Document(
                # ì›ë³¸ textë¥¼ summarizeë¥¼ í•´ì„œ ë³´ë‚´ëŠ” ë°©ë²•-LLM ì†ë„ê°œì„  í…ŒìŠ¤íŠ¸
                page_content=TextSummarizer.summarize(result['page_content'], 5),
                # ì›ë³¸ textë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬í•˜ëŠ” ë°©ë²•
                # page_content=result['page_content'],
                metadata={"score": result['score'], **result['metadata']}
            )            
            docs.append(doc)
        # ë””ë²„ê¹…ì„ ìœ„í•œ ì¶œë ¥
        #for doc in docs:
        #    print(doc.page_content)
            
        filtered_docs = [doc for doc in docs]
        
        if selected_sources:
            filtered_docs = [doc for doc in filtered_docs if doc.metadata.get('source', 'Unknown') in selected_sources]
        
        return filtered_docs

    def process_query(self, query):
        keyword, parsed_query = self.parse_input(query)
        if (keyword == "f" or st.session_state.apply_filter) and st.session_state.rag_usage == "Rag+LLM":
            self.process_filtered_query(parsed_query)
        else:
            self.process_regular_query(parsed_query)

    def process_filtered_query(self, query):
        if st.session_state.selected_sources:
            docs = self.db_manager.get_documents_by_source(self.collection_name, st.session_state.selected_sources)
            if docs:
                self.generate_response(docs, query)
            else:
                st.write("No documents found for the selected sources.")
        else:
            st.info("ê²€ìƒ‰í•  ë¬¸ì„œë¥¼ ì„ íƒí•˜ì„¸ìš”!")

    def process_regular_query(self, query):
        if st.session_state.RagUsage == "Rag+LLM":
            with st.spinner('Rag+LLM ë‹µë³€ ì‚¬ì „ ì¤€ë¹„ì¤‘...'):
                docs = self.perform_search(query, self.db_manager, self.collection_name, st.session_state.selected_sources)                
            if docs:
                self.generate_response(docs, query)                
                self.ui.display_search_results(docs)
            else:
                self.fallback_to_llm(query)
        else:
            self.generate_response(None, query)            
            
    def generate_response(self, docs, query):
        try:
            # Create the prompt template
            prompt_template = """
            System: You are an AI assistant that answers questions using only the provided information. Do not include any content not present in the given information. If the information is insufficient, say "I cannot answer with the given information."

            Context: {context}

            Human: {question}

            AI: """

            # Prepare the context from the retrieved documents
            context = "\n".join([doc.page_content for doc in docs])

            # Create the prompt
            prompt = prompt_template.format(context=context, question=query)

            # Load the QA chain
            chain = load_qa_chain(self.llm, chain_type="stuff", verbose=True)

            with st.spinner('AI ë‹µë³€ì„ ê¸°ë‹¤ë¦¬ê³  ìˆëŠ”ì¤‘...'):
                # Run the chain with the prepared prompt
                response = chain.run(input_documents=docs or "", question=prompt)
                st.session_state.messages.append(ChatMessage(role="assistant", content=response))
        except Exception as e:
            error_message = f"LLM ì„œë²„ì—°ê²° ì˜¤ë¥˜ ë°œìƒ: {e}"
            st.error(error_message)
            st.session_state.messages.append(ChatMessage(role="assistant", content=error_message))            

    
    def fallback_to_llm(self, query):
        st.info("ì„ íƒí•œ ì†ŒìŠ¤ë‚˜ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ LLMì— ì§ˆì˜í•©ë‹ˆë‹¤.")
        self.generate_response(None, query)

    def show_pdf(self, file_path):
        file_path = os.path.join(self.persist_directory, file_path)
        with open(file_path, "rb") as f:
            base64_pdf = base64.b64encode(f.read()).decode('utf-8')
        pdf_display = f'<iframe src="data:application/pdf;base64,{base64_pdf}" width="800" height="800" type="application/pdf"></iframe>'
        st.markdown(pdf_display, unsafe_allow_html=True)

    def run(self):
        self.ui.setup_main_ui()
        with st.sidebar:
            self.ui.setup_sidebar()
        self.ui.display_chat_interface()
        
if __name__ == '__main__':
    if platform.system() != 'Windows':
        print("ì´ ì• í”Œë¦¬ì¼€ì´ì…˜ì€ Windowsì—ì„œë§Œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    app = RAGChatApp()
    app.run()
    